{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "## Transductive vs Inductive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'inférence transductive est dérivée des cas d'apprentissage observés qui correspondent à des cas de test spécifiques. L'inférence inductive est dérivée de cas d'apprentissage qui correspondent à des règles générales qui ne sont ensuite appliquées qu'aux cas de test.  \n",
    "\n",
    "Un exemple : imaginez que vous avez un ensemble de données qui n'est que partiellement étiqueté. Certaines choses sont des 'enregistrements', d'autres des 'cds' et d'autres sont vierges. Votre travail consiste à fournir des étiquettes pour les blancs. Si vous choisissez une approche inductive, vous entraîneriez un modèle à la recherche de « records » et « cds », et appliqueriez ces étiquettes à vos données non étiquetées. Cette approche aura du mal à classer les choses qui sont en fait des « cassettes ». Une approche transductive, en revanche, gère plus efficacement ces données inconnues car elle permet de regrouper des éléments similaires, puis d'appliquer une étiquette à un groupe. Dans ce cas, les clusters peuvent refléter des « choses musicales rondes » et des « choses musicales carrées »."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non flat vs flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dérivée de la terminologie mathématique, la géométrie non plate par rapport à la géométrie plate fait référence à la mesure des distances entre les points par des méthodes géométriques « plates » (euclidiennes) ou « non plates » (non euclidiennes).  \n",
    "\n",
    "« Plat » dans ce contexte fait référence à la géométrie euclidienne (dont certaines parties sont enseignées en tant que géométrie « plane »), et non-plat fait référence à la géométrie non-euclidienne. Qu'est-ce que la géométrie a à voir avec l'apprentissage automatique ? Eh bien, en tant que deux domaines enracinés dans les mathématiques, il doit y avoir un moyen commun de mesurer les distances entre les points dans les clusters, et cela peut être fait de manière \"plate\" ou \"non plate\", selon la nature des données . Les distances euclidiennes sont mesurées comme la longueur d'un segment de ligne entre deux points. Les distances non euclidiennes sont mesurées le long d'une courbe. Si vos données, visualisées, semblent ne pas exister dans un avion, vous devrez peut-être utiliser un algorithme spécialisé pour les gérer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances\n",
    "\n",
    "Les clusters sont définis par leur matrice de distance, par ex. les distances entre les points. Cette distance peut être mesurée de plusieurs manières. Les clusters euclidiens sont définis par la moyenne des valeurs des points et contiennent un « centre de gravité » ou un point central. Les distances sont ainsi mesurées par la distance à ce centroïde. Les distances non-euclidiennes se réfèrent aux « clusters », le point le plus proche des autres points. Les clusters à leur tour peuvent être définis de diverses manières.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contraintes\n",
    "\n",
    "Le clustering contraint introduit l'apprentissage « semi-supervisé » dans cette méthode non supervisée. Les relations entre les points sont signalées comme « impossible à lier » ou « à lier obligatoirement », de sorte que certaines règles sont imposées sur l'ensemble de données.  \n",
    "\n",
    "Un exemple : si un algorithme est libéré sur un lot de données non étiquetées ou semi-étiquetées, les clusters qu'il produit peuvent être de mauvaise qualité. Dans l'exemple ci-dessus, les clusters peuvent regrouper les « choses musicales rondes » et les « choses musicales carrées » et les « choses triangulaires » et les « cookies ». Si certaines contraintes ou règles à suivre (\"l'article doit être en plastique\", \"l'article doit être capable de produire de la musique\"), cela peut aider à \"contraindre\" l'algorithme à faire de meilleurs choix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Densité\n",
    "\n",
    "Les données « bruyantes » sont considérées comme « denses ». Les distances entre les points de chacun de ses clusters peuvent s'avérer, à l'examen, plus ou moins denses, ou « encombrées » et ces données doivent donc être analysées avec la méthode de clustering appropriée. Cet article démontre la différence entre l'utilisation des algorithmes de clustering K-Means et HDBSCAN pour explorer un ensemble de données bruyant avec une densité de cluster inégale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Algorithme de clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
